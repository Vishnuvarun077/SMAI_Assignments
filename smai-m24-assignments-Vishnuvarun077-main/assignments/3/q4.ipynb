{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder + KNN Results:\n",
      "Accuracy: 0.2954\n",
      "Precision: 0.2963\n",
      "Recall: 0.2954\n",
      "F1 Score: 0.2885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srivishnuvarun/.local/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP Classifier Results:\n",
      "Accuracy: 0.9426\n",
      "Precision: 0.9435\n",
      "Recall: 0.9426\n",
      "F1 Score: 0.9427\n",
      "\n",
      "Comparison:\n",
      "AutoEncoder + KNN F1 Score: 0.2885\n",
      "MLP Classifier F1 Score: 0.9427\n",
      "Difference: 0.6542\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -709, 709)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i])))\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.layer_outputs = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.layer_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.activation(z)\n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        delta = self.layer_outputs[-1] - y\n",
    "        gradients = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.layer_outputs[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.layer_outputs[i])\n",
    "        \n",
    "        return list(reversed(gradients))\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        for i, (dW, db) in enumerate(gradients):\n",
    "            self.weights[i] -= self.learning_rate * dW\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                self.forward_propagation(X_batch)\n",
    "                gradients = self.backward_propagation(X_batch, y_batch)\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.forward_propagation(X) - y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        encoder_sizes = hidden_sizes + [latent_size]\n",
    "        decoder_sizes = hidden_sizes[::-1] + [input_size]\n",
    "        \n",
    "        self.encoder = MLP(input_size, encoder_sizes[:-1], encoder_sizes[-1])\n",
    "        self.decoder = MLP(latent_size, decoder_sizes[:-1], decoder_sizes[-1])\n",
    "    \n",
    "    def fit(self, X, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                latent = self.encoder.forward_propagation(X_batch)\n",
    "                reconstructed = self.decoder.forward_propagation(latent)\n",
    "                \n",
    "                # Backward pass\n",
    "                decoder_gradients = self.decoder.backward_propagation(latent, X_batch)\n",
    "                encoder_gradients = self.encoder.backward_propagation(X_batch, latent)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.decoder.update_parameters(decoder_gradients)\n",
    "                self.encoder.update_parameters(encoder_gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.decoder.forward_propagation(self.encoder.forward_propagation(X)) - X) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_latent(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            most_common = np.bincount(k_nearest_labels).argmax()\n",
    "            predictions.append(most_common)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../../data/external/spotify.csv')  \n",
    "X = data.drop('track_genre', axis=1)\n",
    "y = data['track_genre']\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"AutoEncoder + KNN Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train MLP classifier\n",
    "input_size = X_train.shape[1]\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(64, 32), activation='logistic', max_iter=1000)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlp = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "precision_mlp = precision_score(y_test, y_pred_mlp, average='weighted')\n",
    "recall_mlp = recall_score(y_test, y_pred_mlp, average='weighted')\n",
    "f1_mlp = f1_score(y_test, y_pred_mlp, average='weighted')\n",
    "\n",
    "print(\"\\nMLP Classifier Results:\")\n",
    "print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "print(f\"Precision: {precision_mlp:.4f}\")\n",
    "print(f\"Recall: {recall_mlp:.4f}\")\n",
    "print(f\"F1 Score: {f1_mlp:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"AutoEncoder + KNN F1 Score: {f1:.4f}\")\n",
    "print(f\"MLP Classifier F1 Score: {f1_mlp:.4f}\")\n",
    "print(f\"Difference: {abs(f1 - f1_mlp):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing KNN on full dataset...\n",
      "\n",
      "Performing KNN on PCA-reduced dataset...\n",
      "\n",
      "---- KNN Performance on Full Dataset ----\n",
      "Accuracy: 0.51\n",
      "Precision: 0.51\n",
      "Recall: 0.51\n",
      "F1 Score (Macro): 0.50\n",
      "F1 Score (Micro): 0.51\n",
      "\n",
      "---- KNN Performance on PCA-Reduced Dataset ----\n",
      "Accuracy: 0.13\n",
      "Precision: 0.12\n",
      "Recall: 0.13\n",
      "F1 Score (Macro): 0.12\n",
      "F1 Score (Micro): 0.13\n",
      "\n",
      "Training AutoEncoder on PCA-reduced dataset...\n",
      "Epoch 0, Loss: 0.0257\n",
      "Epoch 100, Loss: 0.0064\n",
      "Epoch 200, Loss: 0.0056\n",
      "Epoch 300, Loss: 0.0052\n",
      "Epoch 400, Loss: 0.0049\n",
      "Epoch 500, Loss: 0.0047\n",
      "Epoch 600, Loss: 0.0046\n",
      "Epoch 700, Loss: 0.0045\n",
      "Epoch 800, Loss: 0.0044\n",
      "Epoch 900, Loss: 0.0043\n",
      "\n",
      "Performing KNN on AutoEncoder latent representation...\n",
      "\n",
      "---- KNN Performance on AutoEncoder Latent Representation ----\n",
      "Accuracy: 0.12\n",
      "Precision: 0.11\n",
      "Recall: 0.12\n",
      "F1 Score (Macro): 0.11\n",
      "F1 Score (Micro): 0.12\n",
      "\n",
      "Training MLP classifier on original dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,32) and (1,32) not aligned: 32 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 359\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDifference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(latent_f1_macro\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mf1_mlp)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[43mQ9_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 334\u001b[0m, in \u001b[0;36mQ9_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m input_size \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    333\u001b[0m mlp_classifier \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39minput_size, hidden_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m], output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m \u001b[43mmlp_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    337\u001b[0m y_pred_mlp \u001b[38;5;241m=\u001b[39m mlp_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, X, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(X_batch)\n\u001b[0;32m---> 79\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_parameters(gradients)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mMLP.backward_propagation\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m     gradients\u001b[38;5;241m.\u001b[39mappend((dW, db))\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         delta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_outputs[i])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(gradients))\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (32,32) and (1,32) not aligned: 32 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -709, 709)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]) * np.sqrt(2. / layer_sizes[i-1]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i])))\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.layer_outputs = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.layer_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                a = self.activation(z)\n",
    "            else:\n",
    "                a = sigmoid(z) if self.output_size == 1 else z\n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        delta = self.layer_outputs[-1] - y\n",
    "        gradients = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.layer_outputs[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.layer_outputs[i])\n",
    "        \n",
    "        return list(reversed(gradients))\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        for i, (dW, db) in enumerate(gradients):\n",
    "            self.weights[i] -= self.learning_rate * dW\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                self.forward_propagation(X_batch)\n",
    "                gradients = self.backward_propagation(X_batch, y_batch)\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.forward_propagation(X) - y) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        encoder_sizes = hidden_sizes + [latent_size]\n",
    "        decoder_sizes = hidden_sizes[::-1] + [input_size]\n",
    "        \n",
    "        self.encoder = MLP(input_size, encoder_sizes[:-1], encoder_sizes[-1])\n",
    "        self.decoder = MLP(latent_size, decoder_sizes[:-1], decoder_sizes[-1])\n",
    "    \n",
    "    def fit(self, X, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                latent = self.encoder.forward_propagation(X_batch)\n",
    "                reconstructed = self.decoder.forward_propagation(latent)\n",
    "                \n",
    "                # Backward pass\n",
    "                decoder_gradients = self.decoder.backward_propagation(latent, X_batch)\n",
    "                encoder_gradients = self.encoder.backward_propagation(X_batch, latent)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.decoder.update_parameters(decoder_gradients)\n",
    "                self.encoder.update_parameters(encoder_gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.decoder.forward_propagation(self.encoder.forward_propagation(X)) - X) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_latent(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', batch_size=100):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_functions = {\n",
    "            'euclidean': self.euclidean_distance,\n",
    "            'manhattan': self.manhattan_distance,\n",
    "            'cosine': self.cosine_distance\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        predictions = np.zeros(num_samples, dtype=self.y_train.dtype)\n",
    "        \n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch = X[i:i+self.batch_size]\n",
    "            distances = self.distance_functions[self.distance_metric](batch, self.X_train)\n",
    "            k_indices = np.argpartition(distances, self.k, axis=1)[:, :self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            batch_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=k_nearest_labels)\n",
    "            predictions[i:i+self.batch_size] = batch_predictions\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_distance(X1, X2):\n",
    "        return np.sqrt(np.sum((X1[:, np.newaxis, :] - X2) ** 2, axis=2))\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan_distance(X1, X2):\n",
    "        return np.sum(np.abs(X1[:, np.newaxis, :] - X2), axis=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_distance(X1, X2):\n",
    "        dot_product = np.einsum('ijk,jk->ij', X1[:, np.newaxis, :], X2)\n",
    "        norm_X1 = np.linalg.norm(X1, axis=1)\n",
    "        norm_X2 = np.linalg.norm(X2, axis=1)\n",
    "        return 1 - (dot_product / (norm_X1[:, np.newaxis] * norm_X2))\n",
    "\n",
    "def load_and_preprocess_data(file_path, target_column):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop([target_column], axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Convert non-numeric columns to numeric where possible\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Drop columns that cannot be converted to numeric\n",
    "    X = X.select_dtypes(include=[np.number]).dropna(axis=1, how='any')\n",
    "\n",
    "    # Normalize the features (Min-Max scaling)\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    # Handle categorical labels\n",
    "    if y.dtype == 'object':\n",
    "        label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "        y = y.map(label_mapping)\n",
    "    \n",
    "    return X.values, y.values\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    train_indices = indices[:-test_size]\n",
    "    test_indices = indices[-test_size:]\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Center the data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "\n",
    "        # Sort eigenvectors by decreasing eigenvalues\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Store the first n_components eigenvectors\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if mean has been computed\n",
    "        if self.mean is None:\n",
    "            raise ValueError(\"PCA has not been fitted. Call fit() before using transform().\")\n",
    "        \n",
    "        # Center the data\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Project the data onto the principal components\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "def knn_classification(X_train, X_test, y_train, y_test, k, distance_metric):\n",
    "    knn = KNN(k=k, distance_metric=distance_metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate performance\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    return accuracy, precision, recall, f1_macro, f1_micro\n",
    "\n",
    "def Q9_main():\n",
    "    # Load the dataset\n",
    "    file_path = '../../data/external/spotify.csv'  # Update with your dataset path\n",
    "    target_column = 'track_genre'  # Replace with the actual target column\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y = load_and_preprocess_data(file_path, target_column)\n",
    "    \n",
    "    # Step 1: Perform PCA\n",
    "    optimal_components = 3\n",
    "    pca = PCA(n_components=optimal_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # Split the data into training and test sets using custom function\n",
    "    X_train, X_test, y_train, y_test = custom_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_reduced, X_test_reduced, _, _ = custom_train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 2: Apply KNN with the best {k, distance metric} pair (k=19, distance='manhattan')\n",
    "    k_best = 19\n",
    "    distance_best = 'manhattan'\n",
    "    \n",
    "    print(\"\\nPerforming KNN on full dataset...\")\n",
    "    full_accuracy, full_precision, full_recall, full_f1_macro, full_f1_micro = knn_classification(\n",
    "        X_train, X_test, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    print(\"\\nPerforming KNN on PCA-reduced dataset...\")\n",
    "    reduced_accuracy, reduced_precision, reduced_recall, reduced_f1_macro, reduced_f1_micro = knn_classification(\n",
    "        X_train_reduced, X_test_reduced, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    # Step 3: Print evaluation metrics for full and reduced datasets\n",
    "    print(\"\\n---- KNN Performance on Full Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(full_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(full_precision))\n",
    "    print(\"Recall: {:.2f}\".format(full_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(full_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(full_f1_micro))\n",
    "    \n",
    "    print(\"\\n---- KNN Performance on PCA-Reduced Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(reduced_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(reduced_precision))\n",
    "    print(\"Recall: {:.2f}\".format(reduced_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(reduced_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(reduced_f1_micro))\n",
    "    \n",
    "    # Train AutoEncoder on PCA-reduced dataset\n",
    "    print(\"\\nTraining AutoEncoder on PCA-reduced dataset...\")\n",
    "    autoencoder = AutoEncoder(input_size=optimal_components, hidden_sizes=[64, 32], latent_size=optimal_components)\n",
    "    autoencoder.fit(X_train_reduced, epochs=1000, batch_size=32)\n",
    "    \n",
    "    # Get latent representation\n",
    "    X_train_latent = autoencoder.get_latent(X_train_reduced)\n",
    "    X_test_latent = autoencoder.get_latent(X_test_reduced)\n",
    "    \n",
    "    # Apply KNN on latent representation\n",
    "    print(\"\\nPerforming KNN on AutoEncoder latent representation...\")\n",
    "    latent_accuracy, latent_precision, latent_recall, latent_f1_macro, latent_f1_micro = knn_classification(\n",
    "        X_train_latent, X_test_latent, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    # Print evaluation metrics for latent representation\n",
    "    print(\"\\n---- KNN Performance on AutoEncoder Latent Representation ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(latent_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(latent_precision))\n",
    "    print(\"Recall: {:.2f}\".format(latent_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(latent_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(latent_f1_micro))\n",
    "    \n",
    "    # Train MLP classifier on original dataset\n",
    "    print(\"\\nTraining MLP classifier on original dataset...\")\n",
    "    input_size = X_train.shape[1]\n",
    "    mlp_classifier = MLP(input_size=input_size, hidden_sizes=[64, 32], output_size=1, activation='sigmoid', learning_rate=0.01)\n",
    "    mlp_classifier.fit(X_train, y_train, epochs=1000, batch_size=32)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_mlp = mlp_classifier.predict(X_test)\n",
    "    y_pred_mlp = (y_pred_mlp > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "    precision_mlp = precision_score(y_test, y_pred_mlp, average='weighted')\n",
    "    recall_mlp = recall_score(y_test, y_pred_mlp, average='weighted')\n",
    "    f1_mlp = f1_score(y_test, y_pred_mlp, average='weighted')\n",
    "    \n",
    "    print(\"\\nMLP Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "    print(f\"Precision: {precision_mlp:.4f}\")\n",
    "    print(f\"Recall: {recall_mlp:.4f}\")\n",
    "    print(f\"F1 Score: {f1_mlp:.4f}\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"AutoEncoder + KNN F1 Score: {latent_f1_macro:.4f}\")\n",
    "    print(f\"MLP Classifier F1 Score: {f1_mlp:.4f}\")\n",
    "    print(f\"Difference: {abs(latent_f1_macro - f1_mlp):.4f}\")\n",
    "\n",
    "# Run the main function\n",
    "Q9_main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing KNN on full dataset...\n",
      "\n",
      "Performing KNN on PCA-reduced dataset...\n",
      "\n",
      "---- KNN Performance on Full Dataset ----\n",
      "Accuracy: 0.51\n",
      "Precision: 0.51\n",
      "Recall: 0.51\n",
      "F1 Score (Macro): 0.50\n",
      "F1 Score (Micro): 0.51\n",
      "\n",
      "---- KNN Performance on PCA-Reduced Dataset ----\n",
      "Accuracy: 0.13\n",
      "Precision: 0.12\n",
      "Recall: 0.13\n",
      "F1 Score (Macro): 0.12\n",
      "F1 Score (Micro): 0.13\n",
      "\n",
      "Training AutoEncoder on PCA-reduced dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,3) (96,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 359\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDifference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(latent_f1_macro\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mf1_mlp)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[43mQ9_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 311\u001b[0m, in \u001b[0;36mQ9_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining AutoEncoder on PCA-reduced dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m AutoEncoder(input_size\u001b[38;5;241m=\u001b[39moptimal_components, hidden_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m], latent_size\u001b[38;5;241m=\u001b[39moptimal_components)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Get latent representation\u001b[39;00m\n\u001b[1;32m    314\u001b[0m X_train_latent \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mget_latent(X_train_reduced)\n",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m, in \u001b[0;36mAutoEncoder.fit\u001b[0;34m(self, X, epochs, batch_size)\u001b[0m\n\u001b[1;32m    108\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mforward_propagation(latent)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m decoder_gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m encoder_gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mbackward_propagation(X_batch, latent)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mMLP.backward_propagation\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     54\u001b[0m     m \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights))):\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,3) (96,1) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -709, 709)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]) * np.sqrt(2. / layer_sizes[i-1]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i])))\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.layer_outputs = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.layer_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                a = self.activation(z)\n",
    "            else:\n",
    "                a = sigmoid(z) if self.output_size == 1 else z\n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        delta = self.layer_outputs[-1] - y.reshape(-1, 1)\n",
    "        gradients = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.layer_outputs[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.layer_outputs[i])\n",
    "        \n",
    "        return list(reversed(gradients))\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        for i, (dW, db) in enumerate(gradients):\n",
    "            self.weights[i] -= self.learning_rate * dW\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                self.forward_propagation(X_batch)\n",
    "                gradients = self.backward_propagation(X_batch, y_batch)\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.forward_propagation(X) - y.reshape(-1, 1)) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        encoder_sizes = hidden_sizes + [latent_size]\n",
    "        decoder_sizes = hidden_sizes[::-1] + [input_size]\n",
    "        \n",
    "        self.encoder = MLP(input_size, encoder_sizes[:-1], encoder_sizes[-1])\n",
    "        self.decoder = MLP(latent_size, decoder_sizes[:-1], decoder_sizes[-1])\n",
    "    \n",
    "    def fit(self, X, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                latent = self.encoder.forward_propagation(X_batch)\n",
    "                reconstructed = self.decoder.forward_propagation(latent)\n",
    "                \n",
    "                # Backward pass\n",
    "                decoder_gradients = self.decoder.backward_propagation(latent, X_batch)\n",
    "                encoder_gradients = self.encoder.backward_propagation(X_batch, latent)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.decoder.update_parameters(decoder_gradients)\n",
    "                self.encoder.update_parameters(encoder_gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.decoder.forward_propagation(self.encoder.forward_propagation(X)) - X) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_latent(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', batch_size=100):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_functions = {\n",
    "            'euclidean': self.euclidean_distance,\n",
    "            'manhattan': self.manhattan_distance,\n",
    "            'cosine': self.cosine_distance\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        predictions = np.zeros(num_samples, dtype=self.y_train.dtype)\n",
    "        \n",
    "        for i in range(0, num_samples, self.batch_size):\n",
    "            batch = X[i:i+self.batch_size]\n",
    "            distances = self.distance_functions[self.distance_metric](batch, self.X_train)\n",
    "            k_indices = np.argpartition(distances, self.k, axis=1)[:, :self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            batch_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=k_nearest_labels)\n",
    "            predictions[i:i+self.batch_size] = batch_predictions\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_distance(X1, X2):\n",
    "        return np.sqrt(np.sum((X1[:, np.newaxis, :] - X2) ** 2, axis=2))\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan_distance(X1, X2):\n",
    "        return np.sum(np.abs(X1[:, np.newaxis, :] - X2), axis=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_distance(X1, X2):\n",
    "        dot_product = np.einsum('ijk,jk->ij', X1[:, np.newaxis, :], X2)\n",
    "        norm_X1 = np.linalg.norm(X1, axis=1)\n",
    "        norm_X2 = np.linalg.norm(X2, axis=1)\n",
    "        return 1 - (dot_product / (norm_X1[:, np.newaxis] * norm_X2))\n",
    "\n",
    "def load_and_preprocess_data(file_path, target_column):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop([target_column], axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Convert non-numeric columns to numeric where possible\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Drop columns that cannot be converted to numeric\n",
    "    X = X.select_dtypes(include=[np.number]).dropna(axis=1, how='any')\n",
    "\n",
    "    # Normalize the features (Min-Max scaling)\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    # Handle categorical labels\n",
    "    if y.dtype == 'object':\n",
    "        label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "        y = y.map(label_mapping)\n",
    "    \n",
    "    return X.values, y.values\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    train_indices = indices[:-test_size]\n",
    "    test_indices = indices[-test_size:]\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Center the data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "\n",
    "        # Sort eigenvectors by decreasing eigenvalues\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Store the first n_components eigenvectors\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Check if mean has been computed\n",
    "        if self.mean is None:\n",
    "            raise ValueError(\"PCA has not been fitted. Call fit() before using transform().\")\n",
    "        \n",
    "        # Center the data\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Project the data onto the principal components\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "def knn_classification(X_train, X_test, y_train, y_test, k, distance_metric):\n",
    "    knn = KNN(k=k, distance_metric=distance_metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate performance\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    return accuracy, precision, recall, f1_macro, f1_micro\n",
    "\n",
    "def Q9_main():\n",
    "    # Load the dataset\n",
    "    file_path = '../../data/external/spotify.csv'  # Update with your dataset path\n",
    "    target_column = 'track_genre'  # Replace with the actual target column\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y = load_and_preprocess_data(file_path, target_column)\n",
    "    \n",
    "    # Step 1: Perform PCA\n",
    "    optimal_components = 3\n",
    "    pca = PCA(n_components=optimal_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # Split the data into training and test sets using custom function\n",
    "    X_train, X_test, y_train, y_test = custom_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_reduced, X_test_reduced, _, _ = custom_train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 2: Apply KNN with the best {k, distance metric} pair (k=19, distance='manhattan')\n",
    "    k_best = 19\n",
    "    distance_best = 'manhattan'\n",
    "    \n",
    "    print(\"\\nPerforming KNN on full dataset...\")\n",
    "    full_accuracy, full_precision, full_recall, full_f1_macro, full_f1_micro = knn_classification(\n",
    "        X_train, X_test, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    print(\"\\nPerforming KNN on PCA-reduced dataset...\")\n",
    "    reduced_accuracy, reduced_precision, reduced_recall, reduced_f1_macro, reduced_f1_micro = knn_classification(\n",
    "        X_train_reduced, X_test_reduced, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    # Step 3: Print evaluation metrics for full and reduced datasets\n",
    "    print(\"\\n---- KNN Performance on Full Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(full_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(full_precision))\n",
    "    print(\"Recall: {:.2f}\".format(full_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(full_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(full_f1_micro))\n",
    "    \n",
    "    print(\"\\n---- KNN Performance on PCA-Reduced Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(reduced_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(reduced_precision))\n",
    "    print(\"Recall: {:.2f}\".format(reduced_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(reduced_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(reduced_f1_micro))\n",
    "    \n",
    "    # Train AutoEncoder on PCA-reduced dataset\n",
    "    print(\"\\nTraining AutoEncoder on PCA-reduced dataset...\")\n",
    "    autoencoder = AutoEncoder(input_size=optimal_components, hidden_sizes=[64, 32], latent_size=optimal_components)\n",
    "    autoencoder.fit(X_train_reduced, epochs=1000, batch_size=32)\n",
    "    \n",
    "    # Get latent representation\n",
    "    X_train_latent = autoencoder.get_latent(X_train_reduced)\n",
    "    X_test_latent = autoencoder.get_latent(X_test_reduced)\n",
    "    \n",
    "    # Apply KNN on latent representation\n",
    "    print(\"\\nPerforming KNN on AutoEncoder latent representation...\")\n",
    "    latent_accuracy, latent_precision, latent_recall, latent_f1_macro, latent_f1_micro = knn_classification(\n",
    "        X_train_latent, X_test_latent, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    # Print evaluation metrics for latent representation\n",
    "    print(\"\\n---- KNN Performance on AutoEncoder Latent Representation ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(latent_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(latent_precision))\n",
    "    print(\"Recall: {:.2f}\".format(latent_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(latent_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(latent_f1_micro))\n",
    "    \n",
    "    # Train MLP classifier on original dataset\n",
    "    print(\"\\nTraining MLP classifier on original dataset...\")\n",
    "    input_size = X_train.shape[1]\n",
    "    mlp_classifier = MLP(input_size=input_size, hidden_sizes=[64, 32], output_size=1, activation='sigmoid', learning_rate=0.01)\n",
    "    mlp_classifier.fit(X_train, y_train, epochs=1000, batch_size=32)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_mlp = mlp_classifier.predict(X_test)\n",
    "    y_pred_mlp = (y_pred_mlp > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "    precision_mlp = precision_score(y_test, y_pred_mlp, average='weighted')\n",
    "    recall_mlp = recall_score(y_test, y_pred_mlp, average='weighted')\n",
    "    f1_mlp = f1_score(y_test, y_pred_mlp, average='weighted')\n",
    "    \n",
    "    print(\"\\nMLP Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "    print(f\"Precision: {precision_mlp:.4f}\")\n",
    "    print(f\"Recall: {recall_mlp:.4f}\")\n",
    "    print(f\"F1 Score: {f1_mlp:.4f}\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"AutoEncoder + KNN F1 Score: {latent_f1_macro:.4f}\")\n",
    "    print(f\"MLP Classifier F1 Score: {f1_mlp:.4f}\")\n",
    "    print(f\"Difference: {abs(latent_f1_macro - f1_mlp):.4f}\")\n",
    "\n",
    "# Run the main function\n",
    "Q9_main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 175\u001b[0m\n\u001b[1;32m    172\u001b[0m y_spotify_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_spotify)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# One-hot encode the target labels\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m onehot_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m y_spotify_onehot \u001b[38;5;241m=\u001b[39m onehot_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_spotify_encoded\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Split the data into training, validation, and test sets\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, task='classification', activation='relu', \n",
    "                 learning_rate=0.01, epochs=100, batch_size=32, optimizer='sgd', patience=5, min_delta=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.task = task\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i-1], layer_sizes[i]) * np.sqrt(2. / layer_sizes[i-1]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i])))\n",
    "\n",
    "        self.set_activation(activation)\n",
    "\n",
    "    def set_activation(self, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_derivative = self.sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.tanh\n",
    "            self.activation_derivative = self.tanh_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "            self.activation_derivative = self.relu_derivative\n",
    "        elif activation == 'linear':\n",
    "            self.activation = self.linear\n",
    "            self.activation_derivative = self.linear_derivative\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -709, 709)))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_derivative(x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.layer_outputs = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.layer_outputs[-1], self.weights[i]) + self.biases[i]\n",
    "            if i < len(self.weights) - 1:\n",
    "                a = self.activation(z)\n",
    "            else:\n",
    "                if self.task == 'classification':\n",
    "                    a = self.sigmoid(z)\n",
    "                else:\n",
    "                    a = z\n",
    "            self.layer_outputs.append(a)\n",
    "        return self.layer_outputs[-1]\n",
    "\n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        if self.task == 'classification':\n",
    "            delta = self.layer_outputs[-1] - y\n",
    "        else:\n",
    "            delta = self.layer_outputs[-1] - y.reshape(-1, 1)\n",
    "        gradients = []\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.layer_outputs[i].T, delta) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activation_derivative(self.layer_outputs[i])\n",
    "        return list(reversed(gradients))\n",
    "\n",
    "    def update_parameters(self, gradients):\n",
    "        for i, (dW, db) in enumerate(gradients):\n",
    "            self.weights[i] -= self.learning_rate * dW\n",
    "            self.biases[i] -= self.learning_rate * db\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                batch_X = X[i:i+self.batch_size]\n",
    "                batch_y = y[i:i+self.batch_size]\n",
    "                \n",
    "                y_pred = self.forward_propagation(batch_X)\n",
    "                gradients = self.backward_propagation(batch_X, batch_y)\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            loss = self.compute_loss(X, y)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss = self.compute_loss(X_val, y_val)\n",
    "                self.val_losses.append(val_loss)\n",
    "                \n",
    "                if val_loss < best_val_loss - self.min_delta:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        y_pred = self.forward_propagation(X)\n",
    "        if self.task == 'classification':\n",
    "            return -np.mean(y * np.log(y_pred + 1e-8) + (1 - y) * np.log(1 - y_pred + 1e-8))\n",
    "        else:\n",
    "            return np.mean((y_pred - y.reshape(-1, 1))**2)\n",
    "\n",
    "# Load the Spotify dataset for classification\n",
    "spotify_df = pd.read_csv('../../data/external/spotify.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "X_spotify = spotify_df.drop('track_genre', axis=1)\n",
    "y_spotify = spotify_df['track_genre']\n",
    "\n",
    "# Convert non-numeric columns to numeric where possible\n",
    "X_spotify = X_spotify.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop columns that cannot be converted to numeric\n",
    "X_spotify = X_spotify.select_dtypes(include=[np.number]).dropna(axis=1, how='any')\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_spotify_encoded = label_encoder.fit_transform(y_spotify)\n",
    "\n",
    "# One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_spotify_onehot = onehot_encoder.fit_transform(y_spotify_encoded.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_s, X_temp_s, y_train_s, y_temp_s = train_test_split(X_spotify, y_spotify_onehot, test_size=0.3, random_state=42)\n",
    "X_val_s, X_test_s, y_val_s, y_test_s = train_test_split(X_temp_s, y_temp_s, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler_s = StandardScaler()\n",
    "X_train_s_standardized = scaler_s.fit_transform(X_train_s)\n",
    "X_val_s_standardized = scaler_s.transform(X_val_s)\n",
    "X_test_s_standardized = scaler_s.transform(X_test_s)\n",
    "\n",
    "# Convert back to DataFrame for consistency\n",
    "X_train_s = pd.DataFrame(X_train_s_standardized, columns=X_spotify.columns)\n",
    "X_val_s = pd.DataFrame(X_val_s_standardized, columns=X_spotify.columns)\n",
    "X_test_s = pd.DataFrame(X_test_s_standardized, columns=X_spotify.columns)\n",
    "\n",
    "# Train model for classification\n",
    "output_size = y_spotify_onehot.shape[1]\n",
    "model_classification = MLP(input_size=X_train_s.shape[1], hidden_sizes=[64, 32], output_size=output_size, task='classification', activation='relu', epochs=1000)\n",
    "model_classification.fit(X_train_s, y_train_s, X_val_s, y_val_s)\n",
    "\n",
    "# Evaluate classification model\n",
    "y_pred_classification = model_classification.predict(X_test_s)\n",
    "y_pred_classification_labels = np.argmax(y_pred_classification, axis=1)\n",
    "y_test_labels = np.argmax(y_test_s, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_classification_labels)\n",
    "precision = precision_score(y_test_labels, y_pred_classification_labels, average='weighted')\n",
    "recall = recall_score(y_test_labels, y_pred_classification_labels, average='weighted')\n",
    "f1 = f1_score(y_test_labels, y_pred_classification_labels, average='weighted')\n",
    "\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Precision: {precision:.4f}\")\n",
    "print(f\"Classification Recall: {recall:.4f}\")\n",
    "print(f\"Classification F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 32 into shape (114)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m custom_train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# X_train_reduced, X_test_reduced, _, _ = custom_train_test_split(X_reduced, y, test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Run the MLP classification\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[43mmlp_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mmlp_classification\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     88\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train))\n\u001b[1;32m     89\u001b[0m mlp_classifier \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39minput_size, hidden_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m], output_size\u001b[38;5;241m=\u001b[39moutput_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[43mmlp_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     93\u001b[0m y_pred_mlp \u001b[38;5;241m=\u001b[39m mlp_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, X, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(X_batch)\n\u001b[0;32m---> 75\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_parameters(gradients)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mMLP.backward_propagation\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_propagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     50\u001b[0m     m \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights))):\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 32 into shape (114)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example data (replace with actual data)\n",
    "    # Load the dataset\n",
    "file_path = '../../data/external/spotify.csv'  # Update with your dataset path\n",
    "target_column = 'track_genre'  # Replace with the actual target column\n",
    "    \n",
    "    # Load and preprocess data\n",
    "X, y = load_and_preprocess_data(file_path, target_column)\n",
    "X_train, X_test, y_train, y_test = custom_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train_reduced, X_test_reduced, _, _ = custom_train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Run the MLP classification\n",
    "mlp_classification(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMLP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMLP\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLP  \u001b[38;5;66;03m# Assuming MLP and AutoEncoder are defined in models/MLP/MLP.py\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mKNN\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mKNN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNN  \u001b[38;5;66;03m# Assuming KNN is defined in models/KNN/KNN.py\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPCA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPCA\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA  \u001b[38;5;66;03m# Assuming PCA is defined in models/PCA/PCA.py\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from models.MLP.MLP import MLP  # Assuming MLP and AutoEncoder are defined in models/MLP/MLP.py\n",
    "from models.KNN.KNN import KNN  # Assuming KNN is defined in models/KNN/KNN.py\n",
    "from models.PCA.PCA import PCA  # Assuming PCA is defined in models/PCA/PCA.py\n",
    "\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size, activation='relu', learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Define the encoder and decoder using the MLP class\n",
    "        encoder_sizes = hidden_sizes + [latent_size]\n",
    "        decoder_sizes = hidden_sizes[::-1] + [input_size]\n",
    "        \n",
    "        self.encoder = MLP(input_size, encoder_sizes[:-1], encoder_sizes[-1], activation=activation, learning_rate=learning_rate)\n",
    "        self.decoder = MLP(latent_size, decoder_sizes[:-1], decoder_sizes[-1], activation=activation, learning_rate=learning_rate)\n",
    "    \n",
    "    def fit(self, X, epochs=1000, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                latent = self.encoder.forward_propagation(X_batch)\n",
    "                reconstructed = self.decoder.forward_propagation(latent)\n",
    "                \n",
    "                # Backward pass\n",
    "                decoder_gradients = self.decoder.backward_propagation(latent, X_batch)\n",
    "                encoder_gradients = self.encoder.backward_propagation(X_batch, latent)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.decoder.update_parameters(decoder_gradients)\n",
    "                self.encoder.update_parameters(encoder_gradients)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean((self.decoder.forward_propagation(self.encoder.forward_propagation(X)) - X) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def get_latent(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path, target_column):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop([target_column], axis=1)\n",
    "    y = data[target_column]\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    X = X.select_dtypes(include=[np.number]).dropna(axis=1, how='any')\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    if y.dtype == 'object':\n",
    "        label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "        y = y.map(label_mapping)\n",
    "    return X.values, y.values\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    test_size = int(X.shape[0] * test_size)\n",
    "    train_indices = indices[:-test_size]\n",
    "    test_indices = indices[-test_size:]\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def knn_classification(X_train, X_test, y_train, y_test, k, distance_metric):\n",
    "    knn = KNN(k=k, distance_metric=distance_metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    return accuracy, precision, recall, f1_macro, f1_micro\n",
    "\n",
    "def Q9_main():\n",
    "    file_path = '../../data/external/spotify.csv'\n",
    "    target_column = 'track_genre'\n",
    "    X, y = load_and_preprocess_data(file_path, target_column)\n",
    "    optimal_components = 3\n",
    "    pca = PCA(n_components=optimal_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = custom_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_reduced, X_test_reduced, _, _ = custom_train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "    k_best = 19\n",
    "    distance_best = 'manhattan'\n",
    "    \n",
    "    print(\"\\nPerforming KNN on full dataset...\")\n",
    "    full_accuracy, full_precision, full_recall, full_f1_macro, full_f1_micro = knn_classification(\n",
    "        X_train, X_test, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    print(\"\\nPerforming KNN on PCA-reduced dataset...\")\n",
    "    reduced_accuracy, reduced_precision, reduced_recall, reduced_f1_macro, reduced_f1_micro = knn_classification(\n",
    "        X_train_reduced, X_test_reduced, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    print(\"\\n---- KNN Performance on Full Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(full_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(full_precision))\n",
    "    print(\"Recall: {:.2f}\".format(full_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(full_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(full_f1_micro))\n",
    "    \n",
    "    print(\"\\n---- KNN Performance on PCA-Reduced Dataset ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(reduced_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(reduced_precision))\n",
    "    print(\"Recall: {:.2f}\".format(reduced_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(reduced_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(reduced_f1_micro))\n",
    "    \n",
    "    print(\"\\nTraining AutoEncoder on PCA-reduced dataset...\")\n",
    "    autoencoder = AutoEncoder(input_size=optimal_components, hidden_sizes=[64, 32], latent_size=optimal_components)\n",
    "    autoencoder.fit(X_train_reduced, epochs=1000, batch_size=32)\n",
    "    X_train_latent = autoencoder.get_latent(X_train_reduced)\n",
    "    X_test_latent = autoencoder.get_latent(X_test_reduced)\n",
    "    \n",
    "    print(\"\\nPerforming KNN on AutoEncoder latent representation...\")\n",
    "    latent_accuracy, latent_precision, latent_recall, latent_f1_macro, latent_f1_micro = knn_classification(\n",
    "        X_train_latent, X_test_latent, y_train, y_test, k_best, distance_best)\n",
    "    \n",
    "    print(\"\\n---- KNN Performance on AutoEncoder Latent Representation ----\")\n",
    "    print(\"Accuracy: {:.2f}\".format(latent_accuracy))\n",
    "    print(\"Precision: {:.2f}\".format(latent_precision))\n",
    "    print(\"Recall: {:.2f}\".format(latent_recall))\n",
    "    print(\"F1 Score (Macro): {:.2f}\".format(latent_f1_macro))\n",
    "    print(\"F1 Score (Micro): {:.2f}\".format(latent_f1_micro))\n",
    "    \n",
    "    print(\"\\nTraining MLP classifier on original dataset...\")\n",
    "    input_size = X_train.shape[1]\n",
    "    mlp_classifier = MLP(input_size=input_size, hidden_sizes=[64, 32], output_size=1, activation='sigmoid', learning_rate=0.01)\n",
    "    mlp_classifier.fit(X_train, y_train, epochs=1000, batch_size=32)\n",
    "    y_pred_mlp = mlp_classifier.predict(X_test)\n",
    "    y_pred_mlp = (y_pred_mlp > 0.5).astype(int).flatten()\n",
    "    \n",
    "    accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "    precision_mlp = precision_score(y_test, y_pred_mlp, average='weighted')\n",
    "    recall_mlp = recall_score(y_test, y_pred_mlp, average='weighted')\n",
    "    f1_mlp = f1_score(y_test, y_pred_mlp, average='weighted')\n",
    "    \n",
    "    print(\"\\nMLP Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "    print(f\"Precision: {precision_mlp:.4f}\")\n",
    "    print(f\"Recall: {recall_mlp:.4f}\")\n",
    "    print(f\"F1 Score: {f1_mlp:.4f}\")\n",
    "    \n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"AutoEncoder + KNN F1 Score: {latent_f1_macro:.4f}\")\n",
    "    print(f\"MLP Classifier F1 Score: {f1_mlp:.4f}\")\n",
    "    print(f\"Difference: {abs(latent_f1_macro - f1_mlp):.4f}\")\n",
    "\n",
    "# Run the main function\n",
    "Q9_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
