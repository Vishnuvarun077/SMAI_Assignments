{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CRIM          ZN      INDUS      CHAS       NOX        RM  \\\n",
      "mean   3.611874   11.211934  11.083992  0.069959  0.554695  6.284634   \n",
      "std    8.720192   23.388876   6.835896  0.255340  0.115878  0.702617   \n",
      "min    0.006320    0.000000   0.460000  0.000000  0.385000  3.561000   \n",
      "max   88.976200  100.000000  27.740000  1.000000  0.871000  8.780000   \n",
      "\n",
      "             AGE        DIS        RAD         TAX    PTRATIO           B  \\\n",
      "mean   68.518519   3.795043   9.549407  408.237154  18.455534  356.674032   \n",
      "std    27.999513   2.105710   8.707259  168.537116   2.164946   91.294864   \n",
      "min     2.900000   1.129600   1.000000  187.000000  12.600000    0.320000   \n",
      "max   100.000000  12.126500  24.000000  711.000000  22.000000  396.900000   \n",
      "\n",
      "          LSTAT       MEDV  \n",
      "mean  12.715432  22.532806  \n",
      "std    7.155871   9.197104  \n",
      "min    1.730000   5.000000  \n",
      "max   37.970000  50.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('../../data/external/HousingData.csv')\n",
    "\n",
    "# # Describe the dataset\n",
    "# description = data.describe()\n",
    "# print(description)\n",
    "\n",
    "# # Plot the distribution of the target variable\n",
    "# plt.hist(data['MEDV'], bins=30)\n",
    "# plt.title('Distribution of MEDV')\n",
    "# plt.xlabel('MEDV')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Partition the dataset\n",
    "# X = data.drop('MEDV', axis=1)\n",
    "# y = data['MEDV']\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Normalize and standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset and display basic statistics\n",
    "housing_data = pd.read_csv(\"../../data/external/HousingData.csv\")\n",
    "stats_summary = housing_data.agg(['mean', 'std', 'min', 'max'])\n",
    "print(stats_summary)\n",
    "\n",
    "# Plot the distribution of the target variable (MEDV)\n",
    "medv_values = housing_data['MEDV']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(medv_values, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('MEDV')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of MEDV')\n",
    "plt.show()\n",
    "\n",
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "features = housing_data.drop(columns=['MEDV'])\n",
    "target = housing_data['MEDV']\n",
    "target = np.array(target).reshape(-1, 1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Handle missing values by imputing with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_val_imputed = imputer.transform(X_val)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Normalize the features using Min-Max scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_normalized = min_max_scaler.fit_transform(X_train_imputed)\n",
    "X_val_normalized = min_max_scaler.transform(X_val_imputed)\n",
    "X_test_normalized = min_max_scaler.transform(X_test_imputed)\n",
    "\n",
    "# Standardize the features to have mean=0 and std=1 using Z-score scaling\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_standardized = standard_scaler.fit_transform(X_train_imputed)\n",
    "X_val_standardized = standard_scaler.transform(X_val_imputed)\n",
    "X_test_standardized = standard_scaler.transform(X_test_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, num_hidden_layers=1, num_neurons=64, activation='relu', learning_rate=0.01):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = [self.activation] * (num_hidden_layers + 1)\n",
    "        self.activations[-1] = 'linear'  # Use linear activation for regression\n",
    "\n",
    "    def initialize_weights_and_biases(self, input_dim):\n",
    "        layer_sizes = [input_dim] + [self.num_neurons] * self.num_hidden_layers + [1]  # Regression has one output\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            input_size, output_size = layer_sizes[i - 1], layer_sizes[i]\n",
    "            weight = np.random.randn(input_size, output_size) * 0.01\n",
    "            bias = np.zeros((1, output_size))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        loss = 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def activate(self, x, activation_type):\n",
    "        if activation_type == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            x = np.clip(x, -500, 500)\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation_type == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif activation_type == 'linear':\n",
    "            return x\n",
    "\n",
    "    def d_relu(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def d_linear(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        z_values = []\n",
    "        a_values = [x]\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            z = np.dot(a_values[i], self.weights[i]) + self.biases[i]\n",
    "            a = self.activate(z, self.activations[i])\n",
    "            z_values.append(z)\n",
    "            a_values.append(a)\n",
    "\n",
    "        # Output layer with linear activation for regression\n",
    "        z = np.dot(a_values[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = self.activate(z, self.activations[-1])\n",
    "        z_values.append(z)\n",
    "        a_values.append(a)\n",
    "\n",
    "        return z_values, a_values\n",
    "\n",
    "    def backward_propagation(self, x, y):\n",
    "        m = x.shape[0]\n",
    "        z_values, a_values = self.forward_propagation(x)\n",
    "        gradients = [None] * (self.num_hidden_layers + 1)\n",
    "\n",
    "        # Compute gradient of the output layer\n",
    "        gradients[-1] = (a_values[-1] - y) / m\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in reversed(range(self.num_hidden_layers)):\n",
    "            if self.activations[i] == 'relu':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_relu(a_values[i + 1])\n",
    "            elif self.activations[i] == 'sigmoid':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_sigmoid(a_values[i + 1])\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_tanh(a_values[i + 1])\n",
    "            elif self.activations[i] == 'linear':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_linear(a_values[i + 1])\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        dW = [None] * (self.num_hidden_layers + 1)\n",
    "        db = [None] * (self.num_hidden_layers + 1)\n",
    "        for i in range(self.num_hidden_layers + 1):\n",
    "            dW[i] = np.dot(a_values[i].T, gradients[i])\n",
    "            db[i] = np.sum(gradients[i], axis=0)\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def predict(self, x):\n",
    "        _, y_pred = self.forward_propagation(x)\n",
    "        return y_pred[-1]\n",
    "\n",
    "    def sgd(self, x, y, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "\n",
    "        m = x.shape[0]  # Number of training samples\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(m):\n",
    "                # Select a random training sample (Stochastic Gradient Descent)\n",
    "                random_index = np.random.randint(0, m)\n",
    "                x_sample = x[random_index:random_index + 1]\n",
    "                y_sample = y[random_index:random_index + 1]\n",
    "\n",
    "                dW, db = self.backward_propagation(x_sample, y_sample)\n",
    "\n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "\n",
    "                _, y_pred = self.forward_propagation(x)\n",
    "                loss = self.compute_loss(y, y_pred[-1])\n",
    "                wandb.log({f\"Loss Optimizer=SGD Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"SGD Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "        print(\"SGD Training complete!\")\n",
    "        \n",
    "    def batch_gradient_descent(self, x, y, batch_size=32, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "        \n",
    "        m = x.shape[0]  # Number of training samples\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                dW, db = self.backward_propagation(x_batch, y_batch)\n",
    "                \n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            _, y_pred = self.forward_propagation(x)\n",
    "            loss = self.compute_loss(y, y_pred[-1])\n",
    "            wandb.log({f\"Loss Optimizer=Batch Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Batch Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(\"Batch Training complete!\")\n",
    "\n",
    "    def mini_batch_gradient_descent(self, x, y, batch_size=32, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "        \n",
    "        m = x.shape[0]  # Number of training samples\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            x_shuffled = x[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):\n",
    "                x_batch = x_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                dW, db = self.backward_propagation(x_batch, y_batch)\n",
    "                \n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            _, y_pred = self.forward_propagation(x)\n",
    "            loss = self.compute_loss(y, y_pred[-1])\n",
    "            wandb.log({f\"Loss Optimizer=Mini_Batch Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Mini_Batch Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(\"Mini_Batch Training complete!\")\n",
    "    \n",
    "    def evaluate_model(self,x,y):\n",
    "        _, y_pred = self.forward_propagation(x)\n",
    "        loss = self.compute_loss(y, y_pred[-1])\n",
    "        wandb.log({f\"Loss Optimizer=Mini_Batch Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/srivishnuvarun/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/wandb/run-20241009_222856-vq3s4tvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing/runs/vq3s4tvr' target=\"_blank\">proud-dew-1</a></strong> to <a href='https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing' target=\"_blank\">https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing/runs/vq3s4tvr' target=\"_blank\">https://wandb.ai/vishnuvarun-iiit-hyderabad/regression_example_testing/runs/vq3s4tvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Epoch 100/1000, Loss: 5.5431\n",
      "SGD Epoch 200/1000, Loss: 3.3569\n",
      "SGD Epoch 300/1000, Loss: 3.7936\n",
      "SGD Epoch 400/1000, Loss: 3.6433\n",
      "SGD Epoch 500/1000, Loss: 3.7231\n",
      "SGD Epoch 600/1000, Loss: 3.3090\n",
      "SGD Epoch 700/1000, Loss: 3.2350\n",
      "SGD Epoch 800/1000, Loss: 3.4893\n",
      "SGD Epoch 900/1000, Loss: 3.2599\n",
      "SGD Epoch 1000/1000, Loss: 3.4401\n",
      "SGD Training complete!\n",
      "Hyperparameters for the Best Model:\n",
      "learning_rate: 0.01\n",
      "num_epochs: 1000\n",
      "num_hidden_layers: 2\n",
      "num_neurons: 5\n",
      "activation: sigmoid\n",
      "optimizer: sgd\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+-------------------+\n",
      "| Learning Rate | Number of Epochs | Number of Hidden Layers | Number of Neurons | Activation Function | Optimizer |        MSE         |        RMSE       |     R-Squared     |\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+-------------------+\n",
      "|      0.01     |       1000       |            2            |         5         |       sigmoid       |    sgd    | 12.360342285948096 | 3.515727845830518 | 0.821819991038166 |\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+-------------------+\n",
      "Regression Metrics on Test Set:\n",
      "MSE: 12.7768\n",
      "RMSE: 3.5745\n",
      "R-squared: 0.8392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 120\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Finish the W&B run\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:452\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2157\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[1;32m   2146\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   2149\u001b[0m \n\u001b[1;32m   2150\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2191\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2440\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2437\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   2443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2682\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2679\u001b[0m exit_handle\u001b[38;5;241m.\u001b[39madd_probe(on_probe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_probe_exit)\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;66;03m# wait for the exit to complete\u001b[39;00m\n\u001b[0;32m-> 2682\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mexit_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_exit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n\u001b[1;32m   2685\u001b[0m \u001b[38;5;66;03m# wait for them, it's ok to do this serially but this can be improved\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib64/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "wandb.init(project=\"regression_example_testing\", entity=\"vishnuvarun-iiit-hyderabad\")\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "# learning_rates = [0.01, 0.001]\n",
    "# num_epochs = [500, 1000]\n",
    "# num_hidden_layers = [1, 2]\n",
    "# num_neurons = [64, 128]\n",
    "# activations = ['relu', 'sigmoid','tanh']\n",
    "# optimizers = ['sgd', 'batch_gradient_descent', 'mini_batch_gradient_descent']\n",
    "\n",
    "# # Define hyperparameters to tune\n",
    "learning_rates = [0.01]\n",
    "num_epochs = [1000]\n",
    "num_hidden_layers = [2]\n",
    "num_neurons = [5]\n",
    "activations = ['sigmoid']\n",
    "optimizers = ['sgd']\n",
    "\n",
    "metrics_table = PrettyTable()\n",
    "metrics_table.field_names = [\"Learning Rate\", \"Number of Epochs\", \"Number of Hidden Layers\", \"Number of Neurons\", \"Activation Function\", \"Optimizer\",\"MSE\",\"RMSE\",\"R-Squared\"]\n",
    "\n",
    "\n",
    "# Initialize a dictionary to collect metrics and hyperparameters\n",
    "hyperparameter_metrics = {\n",
    "    'learning_rate': [],\n",
    "    'num_epochs': [],\n",
    "    'num_hidden_layers': [],\n",
    "    'num_neurons': [],\n",
    "    'activation': [],\n",
    "    'optimizer': [],\n",
    "    'MSE': [],\n",
    "    'RMSE': [],\n",
    "    'R2': [],\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "for lr, epochs, num_layers, num_neurons, activation, optimizer in product(learning_rates, num_epochs, num_hidden_layers, num_neurons, activations, optimizers):\n",
    "    \n",
    "    config = {\n",
    "        'learning_rate': lr,\n",
    "        'num_epochs': epochs,\n",
    "        'num_hidden_layers': num_layers,\n",
    "        'num_neurons': num_neurons,\n",
    "        'activation': activation,\n",
    "        'optimizer': optimizer\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        num_hidden_layers=num_layers,\n",
    "        num_neurons=num_neurons,\n",
    "        activation=activation,\n",
    "        learning_rate=lr,\n",
    "    )\n",
    "    model.sgd(X_train_standardized, y_train, num_epochs=epochs)\n",
    "    model.evaluate_model(X_val_standardized,y_val)\n",
    "    y_pred_val = model.predict(X_val_standardized)\n",
    "    \n",
    "    #Calculate and report metrics\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    if mse < best_mse:  # We want to minimize MSE\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        best_hyperparameters = {\n",
    "            'learning_rate': lr,\n",
    "            'num_epochs': epochs,\n",
    "            'num_hidden_layers': num_layers,\n",
    "            'num_neurons': num_neurons,\n",
    "            'activation': activation,\n",
    "            'optimizer': optimizer\n",
    "        }\n",
    "        \n",
    "    metrics_table.add_row([lr, epochs, num_layers, num_neurons, activation, optimizer, mse, rmse, r2])\n",
    "    \n",
    "    hyperparameter_metrics['learning_rate'].append(lr)\n",
    "    hyperparameter_metrics['num_epochs'].append(epochs)\n",
    "    hyperparameter_metrics['num_hidden_layers'].append(num_layers)\n",
    "    hyperparameter_metrics['num_neurons'].append(num_neurons)\n",
    "    hyperparameter_metrics['activation'].append(activation)\n",
    "    hyperparameter_metrics['optimizer'].append(optimizer)\n",
    "    hyperparameter_metrics['MSE'].append(mse)\n",
    "    hyperparameter_metrics['RMSE'].append(rmse)\n",
    "    hyperparameter_metrics['R2'].append(r2)\n",
    "\n",
    "\n",
    "print(\"Hyperparameters for the Best Model:\")\n",
    "for param, value in best_hyperparameters.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(metrics_table)\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_standardized)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = math.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "report = f\"MSE: {mse_test:.4f}\\nRMSE: {rmse_test:.4f}\\nR-squared: {r2_test:.4f}\"\n",
    "\n",
    "print(\"Regression Metrics on Test Set:\")\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
