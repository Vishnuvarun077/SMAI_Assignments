[34m[1mwandb[0m: [33mWARNING[0m Ignored wandb.init() arg project when running a sweep.
[34m[1mwandb[0m: [33mWARNING[0m Config item 'activation' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'optimizer' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).
[34m[1mwandb[0m: [33mWARNING[0m Config item 'batch_size' was locked by 'sweep' (ignored update).
Epoch 0, Loss: 0.2505
Best run: misty-sweep-1
Best configuration: {'epochs': 100, 'optimizer': 'sgd', 'activation': 'relu', 'batch_size': 128, 'hidden_layers': [64, 32], 'learning_rate': 0.07279482930059648}
Best validation accuracy: N/A
[34m[1mwandb[0m: [33mWARNING[0m Ignored wandb.init() arg project when running a sweep.
Epoch 0, Loss: 0.0006
Early stopping at epoch 10
Traceback (most recent call last):
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 308, in <module>
    best_model.train(X_normalized, y_one_hot)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 204, in train
    self.model.fit(X_train, y_train)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 267, in fit
    activations = self._forward_propagation(X)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 223, in _forward_propagation
    z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
ValueError: shapes (914,1598) and (12,64) not aligned: 1598 (dim 1) != 12 (dim 0)
Traceback (most recent call last):
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 308, in <module>
    best_model.train(X_normalized, y_one_hot)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 204, in train
    self.model.fit(X_train, y_train)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 267, in fit
    activations = self._forward_propagation(X)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 223, in _forward_propagation
    z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
ValueError: shapes (914,1598) and (12,64) not aligned: 1598 (dim 1) != 12 (dim 0)
Traceback (most recent call last):
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 308, in <module>
    best_model.train(X_normalized, y_one_hot)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/assignments/3/mlpc.py", line 204, in train
    self.model.fit(X_train, y_train)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 267, in fit
    activations = self._forward_propagation(X)
  File "/home/vishnu/3rd year/SMAI/smai-m24-assignments-Vishnuvarun077/models/MLP/MLP.py", line 223, in _forward_propagation
    z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
ValueError: shapes (914,1598) and (12,64) not aligned: 1598 (dim 1) != 12 (dim 0)
